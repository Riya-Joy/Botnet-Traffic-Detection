# -*- coding: utf-8 -*-
"""Botnet_data4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/186RwWAdLkNCyqrMI6Z64AsaHXcy3vGST
"""

!pip uninstall tensorflow
!pip uninstall keras

!pip install tensorflow-gpu==2.2.0
!pip install -q keras==2.3.1

import tensorflow as tf
print(tf.__version__)

import keras
print(keras.__version__)

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd 
benign=pd.read_csv("/content/drive/MyDrive/Deep_Learning/4.benign.csv")
g_c=pd.read_csv("/content/drive/MyDrive/Deep_Learning/4.gafgyt.combo.csv")
g_j=pd.read_csv("/content/drive/MyDrive/Deep_Learning/4.gafgyt.junk.csv")
g_s=pd.read_csv("/content/drive/MyDrive/Deep_Learning/4.gafgyt.scan.csv")
g_t=pd.read_csv("/content/drive/MyDrive/Deep_Learning/4.gafgyt.tcp.csv")
m_a=pd.read_csv("/content/drive/MyDrive/Deep_Learning/4.mirai.ack.csv")
m_sc=pd.read_csv("/content/drive/MyDrive/Deep_Learning/4.mirai.scan.csv")
m_sy=pd.read_csv("/content/drive/MyDrive/Deep_Learning/4.mirai.syn.csv")
m_u=pd.read_csv("/content/drive/MyDrive/Deep_Learning/4.mirai.udp.csv")
m_u_p=pd.read_csv("/content/drive/MyDrive/Deep_Learning/4.mirai.udpplain.csv")

benign['Class']=0
m_u['Class']=1
g_c['Class']=2
g_j['Class']=3
g_s['Class']=4
g_t['Class']=5
m_a['Class']=6
m_sc['Class']=7
m_sy['Class']=8
m_u_p['Class']=9

data=pd.concat([benign,m_u,g_c,g_j,g_s,g_t,m_a,m_sc,m_sy,m_u_p],
               axis=0, sort=False, ignore_index=True)

print(data['Class'].unique())

ax = data['Class'].value_counts().plot(kind='bar', figsize=(10, 6), fontsize=13, color='#087E8B')
#ax.set_title('Credit card fraud (0 = normal, 1 = fraud)', size=20, pad=30)
#ax.set_ylabel('Number of transactions', fontsize=14)

for i in ax.patches:
    ax.text(i.get_x() + 0.19, i.get_height() + 700, str(round(i.get_height(), 2)), fontsize=15)

data.head()

from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import keras
from keras.models import Sequential,load_model
from keras.layers import Dense,Dropout
from tensorflow.keras.utils import to_categorical
import numpy as np

from keras.layers import Dense, Input, Flatten, Conv1D

from keras.layers import LSTM

data = data.sample(frac=1)

Y = data["Class"]
X = data.drop(columns='Class')

scaler= StandardScaler()
X = scaler.fit_transform(X)

from pickle import dump
dump(scaler, open('scaler.pkl', 'wb'))

Y = Y.values

x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3)

oversample = SMOTE()
x_train, y_train = oversample.fit_resample(x_train, y_train)

from collections import Counter
from matplotlib import pyplot as plt

counter = Counter(y_train)
for k,v in counter.items():
	per = v / len(y_train) * 100
	print('Class=%d, n=%d (%.3f%%)' % (k, v, per))
# plot the distribution
pyplot.bar(counter.keys(), counter.values())
pyplot.show()

y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

x_train.shape

x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1],1))
x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1],1))



model = Sequential()
model.add(Conv1D(filters=64, kernel_size=5, strides=1, padding='same', input_shape = (x_train.shape[1], 1)))
model.add(Conv1D(filters=32, kernel_size=5, strides=1, padding='same'))
model.add(LSTM(32, activation = 'relu', return_sequences=True))
model.add(LSTM(16, return_sequences=True))  # returns a sequence of vectors of dimension 16
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(10,activation='softmax'))
modelName = 'CNN+LSTM'
keras.utils.plot_model(model, './'+modelName+'_Archi.png',show_shapes=True)
model.summary()

model.compile(optimizer="adam",loss="categorical_crossentropy",metrics=['accuracy'])
history2 = model.fit(x_train,y_train,batch_size=1024,epochs = 10,validation_data=(x_test, y_test))

model.save('botnetCNNLSTM.h5')



model1 = Sequential()
model1.add(Dense(35, input_dim=x_train.shape[1], activation='relu'))
model1.add(Dense(26, activation='relu'))
model1.add(Dense(18, activation='relu'))
model1.add(Dense(10, activation='softmax'))

model1.compile(optimizer="adam",loss="categorical_crossentropy",metrics=['accuracy'])
history1 =model1.fit(x_train,y_train,batch_size=1024,epochs = 10,validation_data=(x_test, y_test))

loss_train= history2.history['loss']
loss_val = history2.history['val_loss']
epochs = range(0,10)
plt.plot(epochs, loss_train, 'g', label='Training loss')
plt.plot(epochs, loss_val, 'b', label='validation loss')
plt.title('Training and Validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
#plt.xscale( 'log' )
#plt.axes().set_aspect('equal')
#plt.locator_params(‘loss’,nbins = 10)
plt.legend()
plt.show()

loss_train= history2.history['accuracy']
loss_val = history2.history['val_accuracy']
epochs = range(0,10)
plt.plot(epochs, loss_train, 'g', label='Training accuracy')
plt.plot(epochs, loss_val, 'b', label='validation accuracy')
plt.title('Training and Validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
#plt.xscale( 'log' )
#plt.axes().set_aspect('equal')
#plt.locator_params(‘loss’,nbins = 10)
plt.legend()
plt.show()

len(history1.history['loss'])

len(history1.history['val_loss'])

model1.save('botnetANN.h5')

model = keras.models.load_model('/botnetmodel_v4.h5')

y_pred = model.predict(x_test)
y_pred.shape

y_test = np.argmax(y_test, axis=1)

Y

y_pred = np.argmax(y_pred, axis=1)

y_pred

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

cm = confusion_matrix(y_test, y_pred)
fig, ax = plt.subplots(figsize=(10,10))
sns.heatmap(cm, annot=True, fmt='d')

plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show(block=False)

"""# New section"""

from google.colab import drive
drive.mount('/content/drive')